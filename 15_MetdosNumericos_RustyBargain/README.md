# ğŸš— Rusty Bargain â€“ Used Car Price Prediction

## ğŸ“Œ Project Overview

Rusty Bargain is a second-hand car marketplace developing an application that allows users to quickly estimate the market value of their vehicles.

Using historical listings, technical specifications, equipment versions, and pricing data, this project builds and compares multiple machine learning models to predict the market price of used cars.

The objective is not only to maximize predictive accuracy but also to evaluate:

- Prediction quality
- Training time
- Inference speed

---

## ğŸ¯ Business Objective

The model must:

- Accurately estimate vehicle market value
- Provide fast predictions within the mobile application
- Maintain reasonable training time for periodic model updates

Performance is evaluated using **RMSE (Root Mean Squared Error)**.

---

## ğŸ“Š Dataset Description

**File:** `data/car_data.csv`

### Features

- **DateCrawled** â€” Date when the profile was downloaded  
- **VehicleType** â€” Vehicle body type  
- **RegistrationYear** â€” Registration year  
- **Gearbox** â€” Transmission type  
- **Power** â€” Horsepower (CV)  
- **Model** â€” Vehicle model  
- **Mileage** â€” Mileage (km)  
- **RegistrationMonth** â€” Registration month  
- **FuelType** â€” Fuel type  
- **Brand** â€” Vehicle brand  
- **NotRepaired** â€” Whether vehicle was repaired  
- **DateCreated** â€” Profile creation date  
- **NumberOfPictures** â€” Number of pictures  
- **PostalCode** â€” Owner postal code  
- **LastSeen** â€” Last user activity date  

### Target Variable

- **Price** â€” Vehicle price (EUR)

---

## ğŸ§¹ Data Preprocessing

- Removed missing or unrealistic values  
- Filtered out extreme outliers (invalid registration years, unrealistic prices)  
- Handled categorical variables appropriately  
- Compared encoding strategies depending on model type  
- Split dataset into training and testing sets  

---

## ğŸ¤– Models Implemented

### 1ï¸âƒ£ Linear Regression (Sanity Check)

- Baseline model  
- Used to validate preprocessing and feature pipeline  
- Ensures boosting models outperform a simple linear approach  

---

### 2ï¸âƒ£ Decision Tree Regressor

- Hyperparameter tuning  
- Evaluated overfitting behavior  
- Compared training speed vs predictive performance  

---

### 3ï¸âƒ£ Random Forest Regressor

- Hyperparameter optimization  
- Improved generalization  
- Balanced bias-variance tradeoff  

---

### 4ï¸âƒ£ LightGBM (Gradient Boosting)

- Tuned parameters:
  - `n_estimators`
  - `learning_rate`
  - `max_depth`
- Evaluated computational efficiency  
- Compared against classical tree-based models  

---

### 5ï¸âƒ£ (Optional) XGBoost / CatBoost

- Compared encoding strategies  
- Evaluated computational tradeoffs  
- Analyzed performance differences  

---

## ğŸ“ˆ Model Evaluation

Models were evaluated using:

- **RMSE (Root Mean Squared Error)**
- Training time
- Prediction time

Example timing in Jupyter:

```python
%%time
model.fit(X_train, y_train)
```
---
## ğŸ† Results Summary

### Resultados â€“ ComparaciÃ³n por RMSE mas bajo

| Modelo | Tiempo | RMSE Train | RMSE Test |
|--------|--------|------------|------------|
| Bosque Aleatorio (n_estimators = 100 \| depth = â€¦) | 219.69 | 1041.61 | 1625.77 |
| Bosque Aleatorio (n_estimators = 50 \| depth = 20) | 108.98 | 1047.25 | 1629.97 |
| LightGBM (num_leaves = 60 \| learning_rate = 0.1) | 4.70 | 1585.07 | 1654.84 |
| LightGBM (num_leaves = 60 \| learning_rate = 0.05) | 5.29 | 1664.61 | 1705.09 |
| LightGBM (num_leaves = 30 \| learning_rate = 0.1) | 3.71 | 1669.72 | 1706.37 |


### Resultados â€“ ComparaciÃ³n por tiempo de procesamiento mas bajo

| Modelo | Tiempo | RMSE Train | RMSE Test |
|--------|--------|------------|------------|
| LightGBM (num_leaves = 30 \| learning_rate = 0.1) | 3.71 | 1669.72 | 1706.37 |
| LightGBM (num_leaves = 30 \| learning_rate = 0.05) | 4.00 | 1749.93 | 1772.25 |
| Ãrbol de DecisiÃ³n (max_depth = 5) | 4.16 | 2485.78 | 2491.73 |
| LightGBM (num_leaves = 60 \| learning_rate = 0.1) | 4.70 | 1585.07 | 1654.84 |
| LightGBM (num_leaves = 60 \| learning_rate = 0.05) | 5.29 | 1664.61 | 1705.09 |


LightGBM achieved the best balance between accuracy and computational efficiency.

NOTE: **RMSE (Root Mean Squared Error)** measures the average prediction error of a regression model.  
Lower RMSE indicates better accuracy.  

In this project, RMSE represents the average difference (in euros) between predicted and actual car prices.


---

## ğŸ” Key Insights

- Linear Regression served as a sanity check.
- Tree-based models handled non-linear relationships better.
- Random Forest improved predictive stability.
- Gradient Boosting provided superior performance.
- Encoding strategy significantly impacted results.

---

## ğŸ›  Tech Stack

- Python
- Pandas
- NumPy
- Scikit-learn
- LightGBM
- Matplotlib / Seaborn
- Jupyter Notebook

---

## ğŸ“Œ Future Improvements

- Advanced hyperparameter optimization (Optuna)
- Model explainability (SHAP values)
- Feature importance analysis
- Deployment as an API
- Model monitoring and retraining strategy

---

## ğŸ’¡ Project Highlights

- This project demonstrates:
- Structured model comparison
- Performance benchmarking (accuracy vs speed)
- Hyperparameter tuning
- Business-oriented evaluation
- Reproducible ML workflow
